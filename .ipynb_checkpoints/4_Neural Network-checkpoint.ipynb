{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "# Neural Network Parameters\n",
    "n_hidden_1 = 392\n",
    "n_hidden_2 = 256\n",
    "n_feature = 784\n",
    "n_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : Tensor(\"image_features_1:0\", shape=(?, 784), dtype=float32)\n",
      "Y : Tensor(\"labels_1:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_feature], name='image_features')\n",
    "Y = tf.placeholder('float', [None, n_classes], name='labels')\n",
    "print(\"X :\", X)\n",
    "print(\"Y :\", Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nguye\\Anaconda3\\envs\\env_python3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "weights = {'w1': tf.Variable(tf.random_normal([n_feature, n_hidden_1])),\n",
    "           'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "           'w_out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))}\n",
    "biases = {'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "          'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "          'b_out': tf.Variable(tf.random_normal([n_classes]))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights   =>    {'w1': <tf.Variable 'Variable:0' shape=(784, 392) dtype=float32_ref>, 'w2': <tf.Variable 'Variable_1:0' shape=(392, 256) dtype=float32_ref>, 'w_out': <tf.Variable 'Variable_2:0' shape=(256, 10) dtype=float32_ref>}\n",
      "biases     =>    {'b1': <tf.Variable 'Variable_3:0' shape=(392,) dtype=float32_ref>, 'b2': <tf.Variable 'Variable_4:0' shape=(256,) dtype=float32_ref>, 'b_out': <tf.Variable 'Variable_5:0' shape=(10,) dtype=float32_ref>}\n"
     ]
    }
   ],
   "source": [
    "print(\"weights   =>   \", weights)\n",
    "print(\"biases     =>   \", biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-8170402c613a>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "def neural_network(x):\n",
    "    layer1 = tf.add(tf.matmul(x,weights['w1']),biases['b1']) # layer 1 = 1x392\n",
    "    layer2 = tf.add(tf.matmul(layer1,weights['w2']),biases['b2']) # Layer 2 = 1x256\n",
    "    out_layer = tf.add(tf.matmul(layer2,weights['w_out']),biases['b_out']) # out_layer = 1x10\n",
    "    return out_layer\n",
    "\n",
    "logits = neural_network(X) # X = 15*784 -> logit = 15*10 -> \n",
    "\n",
    "# Compute loss and optimizer\n",
    "loss_op =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels= Y))\n",
    "# backwark\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.1)  \n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate models \n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 13958.709960938\n",
      "Epoch: 0002 cost= 28155.974609375\n",
      "Epoch: 0003 cost= 30573.562500000\n",
      "Epoch: 0004 cost= 15504.360351562\n",
      "Epoch: 0005 cost= 24813.703125000\n",
      "Epoch: 0006 cost= 17859.099609375\n",
      "Epoch: 0007 cost= 13007.443359375\n",
      "Epoch: 0008 cost= 4955.505859375\n",
      "Epoch: 0009 cost= 4542.090332031\n",
      "Epoch: 0010 cost= 8790.020507812\n",
      "Epoch: 0011 cost= 6722.606445312\n",
      "Epoch: 0012 cost= 3541.959472656\n",
      "Epoch: 0013 cost= 2691.530517578\n",
      "Epoch: 0014 cost= 3460.333496094\n",
      "Epoch: 0015 cost= 4387.121582031\n",
      "Epoch: 0016 cost= 4667.999023438\n",
      "Epoch: 0017 cost= 4659.433105469\n",
      "Epoch: 0018 cost= 3492.809326172\n",
      "Epoch: 0019 cost= 2647.688232422\n",
      "Epoch: 0020 cost= 2239.830322266\n",
      "Epoch: 0021 cost= 2111.919433594\n",
      "Epoch: 0022 cost= 2152.465576172\n",
      "Epoch: 0023 cost= 2159.843017578\n",
      "Epoch: 0024 cost= 2119.197753906\n",
      "Epoch: 0025 cost= 2149.847900391\n",
      "Epoch: 0026 cost= 2232.256103516\n",
      "Epoch: 0027 cost= 2156.022460938\n",
      "Epoch: 0028 cost= 1955.437255859\n",
      "Epoch: 0029 cost= 1790.361938477\n",
      "Epoch: 0030 cost= 1652.751708984\n",
      "Epoch: 0031 cost= 1531.926513672\n",
      "Epoch: 0032 cost= 1450.655395508\n",
      "Epoch: 0033 cost= 1406.161254883\n",
      "Epoch: 0034 cost= 1369.609863281\n",
      "Epoch: 0035 cost= 1335.108520508\n",
      "Epoch: 0036 cost= 1306.299438477\n",
      "Epoch: 0037 cost= 1259.916503906\n",
      "Epoch: 0038 cost= 1202.760009766\n",
      "Epoch: 0039 cost= 1148.831665039\n",
      "Epoch: 0040 cost= 1092.743286133\n",
      "Epoch: 0041 cost= 1038.568115234\n",
      "Epoch: 0042 cost= 1001.850524902\n",
      "Epoch: 0043 cost= 978.191589355\n",
      "Epoch: 0044 cost= 948.190246582\n",
      "Epoch: 0045 cost= 908.376159668\n",
      "Epoch: 0046 cost= 868.272460938\n",
      "Epoch: 0047 cost= 834.651184082\n",
      "Epoch: 0048 cost= 808.359497070\n",
      "Epoch: 0049 cost= 784.118103027\n",
      "Epoch: 0050 cost= 754.995666504\n",
      "Epoch: 0051 cost= 724.722167969\n",
      "Epoch: 0052 cost= 700.801757812\n",
      "Epoch: 0053 cost= 681.263977051\n",
      "Epoch: 0054 cost= 661.256408691\n",
      "Epoch: 0055 cost= 643.203552246\n",
      "Epoch: 0056 cost= 625.206420898\n",
      "Epoch: 0057 cost= 601.689025879\n",
      "Epoch: 0058 cost= 576.624633789\n",
      "Epoch: 0059 cost= 555.504882812\n",
      "Epoch: 0060 cost= 536.183288574\n",
      "Epoch: 0061 cost= 518.023986816\n",
      "Epoch: 0062 cost= 503.115631104\n",
      "Epoch: 0063 cost= 490.374023438\n",
      "Epoch: 0064 cost= 478.708160400\n",
      "Epoch: 0065 cost= 466.188446045\n",
      "Epoch: 0066 cost= 452.865661621\n",
      "Epoch: 0067 cost= 439.992187500\n",
      "Epoch: 0068 cost= 426.913879395\n",
      "Epoch: 0069 cost= 414.000122070\n",
      "Epoch: 0070 cost= 401.543670654\n",
      "Epoch: 0071 cost= 389.205444336\n",
      "Epoch: 0072 cost= 377.814147949\n",
      "Epoch: 0073 cost= 368.020172119\n",
      "Epoch: 0074 cost= 358.848266602\n",
      "Epoch: 0075 cost= 348.490386963\n",
      "Epoch: 0076 cost= 338.438079834\n",
      "Epoch: 0077 cost= 329.648376465\n",
      "Epoch: 0078 cost= 321.231628418\n",
      "Epoch: 0079 cost= 312.990478516\n",
      "Epoch: 0080 cost= 304.698394775\n",
      "Epoch: 0081 cost= 296.031951904\n",
      "Epoch: 0082 cost= 287.893096924\n",
      "Epoch: 0083 cost= 280.268402100\n",
      "Epoch: 0084 cost= 272.577514648\n",
      "Epoch: 0085 cost= 265.148773193\n",
      "Epoch: 0086 cost= 258.104309082\n",
      "Epoch: 0087 cost= 251.500778198\n",
      "Epoch: 0088 cost= 245.018051147\n",
      "Epoch: 0089 cost= 238.659561157\n",
      "Epoch: 0090 cost= 232.442092896\n",
      "Epoch: 0091 cost= 226.385604858\n",
      "Epoch: 0092 cost= 220.515594482\n",
      "Epoch: 0093 cost= 214.803131104\n",
      "Epoch: 0094 cost= 209.510360718\n",
      "Epoch: 0095 cost= 204.104324341\n",
      "Epoch: 0096 cost= 198.979156494\n",
      "Epoch: 0097 cost= 193.927932739\n",
      "Epoch: 0098 cost= 189.206359863\n",
      "Epoch: 0099 cost= 184.396545410\n",
      "Epoch: 0100 cost= 179.932983398\n",
      "Epoch: 0101 cost= 175.710479736\n",
      "Epoch: 0102 cost= 171.376327515\n",
      "Epoch: 0103 cost= 167.227783203\n",
      "Epoch: 0104 cost= 163.183395386\n",
      "Epoch: 0105 cost= 159.293777466\n",
      "Epoch: 0106 cost= 155.551040649\n",
      "Epoch: 0107 cost= 152.046432495\n",
      "Epoch: 0108 cost= 149.086471558\n",
      "Epoch: 0109 cost= 148.108184814\n",
      "Epoch: 0110 cost= 156.002105713\n",
      "Epoch: 0111 cost= 216.120834351\n",
      "Epoch: 0112 cost= 740.396606445\n",
      "Epoch: 0113 cost= 1611.261108398\n",
      "Epoch: 0114 cost= 2205.901123047\n",
      "Epoch: 0115 cost= 1418.690917969\n",
      "Epoch: 0116 cost= 840.364807129\n",
      "Epoch: 0117 cost= 2122.908203125\n",
      "Epoch: 0118 cost= 528.139160156\n",
      "Epoch: 0119 cost= 1365.660034180\n",
      "Epoch: 0120 cost= 1269.151367188\n",
      "Epoch: 0121 cost= 885.125061035\n",
      "Epoch: 0122 cost= 631.101989746\n",
      "Epoch: 0123 cost= 582.236145020\n",
      "Epoch: 0124 cost= 875.658996582\n",
      "Epoch: 0125 cost= 917.460815430\n",
      "Epoch: 0126 cost= 660.545288086\n",
      "Epoch: 0127 cost= 550.546203613\n",
      "Epoch: 0128 cost= 541.182006836\n",
      "Epoch: 0129 cost= 549.938232422\n",
      "Epoch: 0130 cost= 575.981384277\n",
      "Epoch: 0131 cost= 581.606750488\n",
      "Epoch: 0132 cost= 542.643127441\n",
      "Epoch: 0133 cost= 471.668304443\n",
      "Epoch: 0134 cost= 417.214080811\n",
      "Epoch: 0135 cost= 398.514251709\n",
      "Epoch: 0136 cost= 402.814697266\n",
      "Epoch: 0137 cost= 405.283752441\n",
      "Epoch: 0138 cost= 393.683258057\n",
      "Epoch: 0139 cost= 368.129699707\n",
      "Epoch: 0140 cost= 340.416717529\n",
      "Epoch: 0141 cost= 321.354370117\n",
      "Epoch: 0142 cost= 313.051635742\n",
      "Epoch: 0143 cost= 306.223785400\n",
      "Epoch: 0144 cost= 293.504394531\n",
      "Epoch: 0145 cost= 282.848327637\n",
      "Epoch: 0146 cost= 274.424346924\n",
      "Epoch: 0147 cost= 261.431823730\n",
      "Epoch: 0148 cost= 245.775024414\n",
      "Epoch: 0149 cost= 234.738830566\n",
      "Epoch: 0150 cost= 229.691467285\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "training_epochs = 100\n",
    "\n",
    "tf.summary.scalar(\"loss_show\", loss_op)\n",
    "tf.summary.scalar(\"accuracy_show\", accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    summary_writer = tf.summary.FileWriter(logdir='./logdir2', graph=tf.get_default_graph())\n",
    "    for epoch in range(training_epochs):\n",
    "        # 1 iterator\n",
    "        #print(sess.run(loss_op))\n",
    "        image_batch, label_batch = mnist.train.next_batch(10000)\n",
    "        sess.run(train_op, feed_dict={X: image_batch, Y: label_batch})\n",
    "#         sess.run(train_op, feed_dict={X: mnist.train.images, Y: mnist.train.labels})\n",
    "        #Display logs per epoch step\n",
    "        #if (epoch+1) % 10 == 0:\n",
    "        loss, summary = sess.run([loss_op, merged_summary_op], feed_dict={X: image_batch, Y: label_batch})\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(loss))\n",
    "        summary_writer.add_summary(summary,epoch)\n",
    "    print(\"Optimization Finished!\")\n",
    "    print (\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    \n",
    "\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
